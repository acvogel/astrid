
\documentclass[letterpaper]{article}



\usepackage{times}
\usepackage{uist}

\begin{document}

% --- Copyright notice ---
\conferenceinfo{UIST'09}{October 4-7, 2009, Victoria, British Columbia, Canada}
\CopyrightYear{2009}
\crdata{978-1-60558-745-5/09/10}

% Uncomment the following line to hide the copyright notice
% \toappear{}
% ------------------------

\bibliographystyle{plain}

%\title{Standard UIST Conference Format:\\
%       Preparing Camera-Ready Submissions}
\title{Voicify: Creating User-Generated Mobile Speech Interfaces by Demonstration}

\author{Adam Vogel \and Arda Kara}

%%
%% Note on formatting authors at different institutions, as shown below:
%% Change width arg (currently 7cm) to parbox commands as needed to
%% accommodate widest lines, taking care not to overflow the 17.8cm line width.
%% Add or delete parboxes for additional authors at different institutions. 
%% If additional authors won't fit in one row, you can add a "\\"  at the
%% end of a parbox's closing "}" to have the next parbox start a new row.
%% Be sure NOT to put any blank lines between parbox commands!
%%

%\author{
%\parbox[t]{9cm}{\centering
%	     {\em Author One}\\
%	     User Interface Laboratory\\
%             ABC Corporation\\
%	     1234 Anywhere Road\\
%	     Anytown, NY 10027 USA\\
%	     +1-212-555-1212\\
%	     one@abc.com}
%\parbox[t]{9cm}{\centering
%	     {\em Author Two}\\
%	     Universit\'{e} de XYZ\\
%	     5678 rue des Parapluies\\
%	     99099 Cr\`{e}me de Menthe, FRANCE\\
%	     +33-12-34-56-78\\
%	     deux@uvw.xyz.fr}
%}

\maketitle

\abstract
%Although voice interfaces exist for the main functions of modern mobile phones, many community-developed 
%applications lack speech support. 
We present \emph{Voicify}, a framework for voice programming by 
demonstration which enables users to build their own speech interfaces.
The user creates their own voice interface by demosntrating a voice command with the corresponding
actions to take on the phone. The phone can then be put in a hands-free mode, where it
responds to previously programmed voice commands. 
Transparency is a key issue, and we discuss the development of a voice-only keyboard for
text entry paired with a custom screen reader for giving feedback to the user.
We conducted a user study which shows that
interfaces created using Voicify rival their touch counterparts.
However, our study also shows that most users find programming by demonstration to be
tedious, and that the natural language processing is too impoverished to be widely applicable.

\section{Introduction}

\section{Related Work}

\section{Programming by Demonstration}
\subsection{UI Capture}

\subsection{Demonstration Interface}

\subsection{Playback Interface}
The Android operating system employs a sandbox-based security framework that prevents any interaction between applications. 
Each application has its own virtual machine, which makes it more difficult to implement a system-wide voice control interface without modifying the OS.
Our workaround was to use the UI testing features of the built-in unit testing tools that come with the Android SDK.
One downside of this approach is that the phone needs to be plugged into the debugging environment during testing; however, 
this didn't come up as a concern from our test users. Listening for the commands was still done on the phone, 
and the Astrid application was modified to receive a voice command upon a button press, look-up the closest matching voice command 
that it has in its database, and send the necessary UI interactions to the test suite using IPC. Our system for receiving commands used 
a word overlap index to find the closest command in its database. Just like the demonstration interface, the user would press a button 
and then utter the voice command they want to use. The phone then echoes the command that it thinks it received and carries out the 
recorded UI interactions. 
 
\begin{enumerate}
\item Screen reader
Android has a built-in screen reader tool called TalkBack that can be turned on as a system-wide setting and allows the user to get 
voice feedback from their interactions by traversing through UI elements using the d-pad on their phones (if available on the model). 
An alternative to TalkBack is a third-party open source tool called Spiel that allows per-application customization through JavaScript. 
Our system did not use either of these packages since they needed the phone's accessibility mode to be turned on, which intervened with 
custom accessibility objects that our code uses. Instead, we implemented a replacement that dispatches d-pad commands to traverse 
through visible UI elements, and simulates accessibility events to gather readable content at each step. This worked well for a uniform 
screen like the list of to-do tasks, but got in the way of user interaction in more complicated screens due to the simulated traversal. 
Therefore we only used the screen reader to read off the list of tasks.

\item Voice keyboard
Upon 
\item Natural language processing for command interpretation
\end{enumerate}

\section{Experimental Evaluation}
\subsection{Experimental Design}

\subsection{Results}

\subsection{Discussion}

\section{Conclusion and Future Work}
Potential improvements: Either infer more about UI elements (not just 
x/y coordinates, but inferring what elements the user is interacting with)
or record the context/screen the user was at during demonstration and only allow 
that command to be dispatched from the same context.


\end{document}
