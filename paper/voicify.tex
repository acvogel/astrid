
\documentclass[letterpaper]{article}



\usepackage{times}
\usepackage{uist}

\begin{document}

% --- Copyright notice ---
\conferenceinfo{UIST'09}{October 4-7, 2009, Victoria, British Columbia, Canada}
\CopyrightYear{2009}
\crdata{978-1-60558-745-5/09/10}

% Uncomment the following line to hide the copyright notice
% \toappear{}
% ------------------------

\bibliographystyle{plain}

%\title{Standard UIST Conference Format:\\
%       Preparing Camera-Ready Submissions}
\title{Voicify: User Demonstrated Voice Interfaces}

%\author{Adam Vogel \and Arda Kara}

%%
%% Note on formatting authors at different institutions, as shown below:
%% Change width arg (currently 7cm) to parbox commands as needed to
%% accommodate widest lines, taking care not to overflow the 17.8cm line width.
%% Add or delete parboxes for additional authors at different institutions. 
%% If additional authors won't fit in one row, you can add a "\\"  at the
%% end of a parbox's closing "}" to have the next parbox start a new row.
%% Be sure NOT to put any blank lines between parbox commands!
%%

\author{
\parbox[t]{9cm}{\centering
	     {\em Adam Vogel}\\
     Computer Science Deparment\\
             Stanford University \\
	     av@cs.stanford.edu}
\parbox[t]{9cm}{\centering
	     {\em Arda Kara}\\
	     Computer Science Department\\
       Stanford University\\
	     ardakara@stanford.edu}
}

\maketitle

\abstract
%Although voice interfaces exist for the main functions of modern mobile phones, many community-developed 
%applications lack speech support. 
We present \emph{Voicify}, a framework for voice programming by 
demonstration which enables users to build their own speech interfaces.
The user creates their own voice interface by demosntrating a voice command with the corresponding
actions to take on the phone. The phone can then be put in a hands-free mode, where it
responds to previously programmed voice commands. 
Transparency is a key issue, and we discuss the development of a voice-only keyboard for
text entry paired with a custom screen reader for giving feedback to the user.
We conducted a user study which shows that
interfaces created using Voicify rival their touch counterparts.
However, our study also shows that most users find programming by demonstration to be
tedious, and that the natural language processing is too impoverished to be widely applicable.

\section{Introduction}
Speech interfaces have been gaining in popularity with the proliferation of mobile devices. Smartphones are
frequently used in hands-free or eyes-free contexts, such as while driving or walking. Modern phones like
the iPhone and Android phones include speech recognition and synthesis in the operating system, allowing
users to make calls and send text messages by voice. However, there is a long tail of community authored
phone applications which have no voice interface. This is a big restriction for the convenience of
hands-free users and the sight disabled. 

To address this problem, we introduce \emph{Voicify}, a framework for creating speech interfaces 
through end-user demonstration. Voicify allows a user to create voice commands for Android
applications through a ``tell and show'' interface: the user first \emph{tells} the phone what 
to do in natural language and then \emph{shows} the phone how to interpret this command.
Once the user has demonstrated commands, the phone can execute commands in a playback mode.

We further explore the efficacy of user demonstration and the usability of the resulting
interface by conducting a user study.  We \emph{voicified} a popular open-source Android todo list
application called \emph{Astrid} (CITE). Using this version of Astrid, we tested the ease and
competence of the demonstration and playback interfaces when compared to the traditional touch
interface. Our results show that the voicified interfaces are usable, with a 60\% command interpretation
success rate, but still fall below the level needed for wide usage. Our user study revealed
several weaknesses in the demonstration interaction which we implemented.

The remainder of the paper summarizes the large body of work on programming by demonstration, details
the design and implementation of Voicify, and discusses the user study.

\section{Related Work}
Some other people did this shit too!

\section{Programming by Demonstration}
The Voicify framework requires three main components: the ability to capture low-level UI events, 
an interface for recording demonstrations, and lastly the ability to playback commands for interpretation.

\subsection{UI Capture}
To record a command demonstration, Voicify requires access to the user interface actions that a user engages in.
Security restrictions on the Android platform make it impossible for a 3rd party application to capture 
touch and key events as this would enable malicious keyloggers to steal sensitive information. 
The Android Accessibility Team (CITE) confirmed that this is impossible to do without modifying the operating system.
To circumvent this problem, we chose an open-source application as a prototype, which allowed us to modify
the program source itself, thus capturing UI events within the application.

The UI design of Android still makes capturing UI events across a whole application a difficult task.
Within each activity, the interface elements, called \emph{views}, are arranged in a tree structure. When the user
touches the screen, the UI event is dispatched to the lowest element in the view tree which spans that portion of the 
screen. It is then passed up the tree, until a UI element consumes it. To capture these touch events,
we programatically traverse the view tree and insert an invisible UI layer at each node. The UI layer
simply captures the UI interactions which pass through it.

Furthermore, some UI widgets like buttons and dialog boxes consume touch events without passing them along
to the activity in which they reside. Complicating matters, these boxes are required to be leaves
of the view tree, which doesn't allow for our trick of inserting invisible layers. To deal with these issues
we overrid the UI widget classes to capture the interactions. This required changing the UI definitions for
each of the screens to use our modified versions. This challenges our goal of making Voicify applicable to
every application but is a necessary hack for this prototype.

Lastly, each screen in an Android application is run in its own process, which pauses when switching between
screens in an application. This makes it difficult to share UI events across activities, which very naturally
happens when demonstrating some commands. To overcome this we implemented a \emph{demonstration service} which runs in the background
of the application and aggregates UI interactions across activities.

When recording UI events, we capture low-level touches and key preses.

%TODO: figure for the demonstration service?

\subsection{Demonstration Interface}

\subsection{Playback Interface}
The Android operating system employs a sandbox-based security framework that prevents any interaction between applications. 
Each application has its own virtual machine, which makes it more difficult to implement a system-wide voice control interface without modifying the OS.
Our workaround was to use the UI testing features of the built-in unit testing tools that come with the Android SDK.
One downside of this approach is that the phone needs to be plugged into the debugging environment during testing; however, 
this didn't come up as a concern from our test users. Listening for the commands was still done on the phone, 
and the Astrid application was modified to receive a voice command upon a button press, look-up the closest matching voice command 
that it has in its database, and send the necessary UI interactions to the test suite using IPC. Our system for receiving commands used 
a word overlap index to find the closest command in its database. Just like the demonstration interface, the user would press a button 
and then utter the voice command they want to use. The phone then echoes the command that it thinks it received and carries out the 
recorded UI interactions. 
 
\begin{enumerate}
\item Screen reader
Android has a built-in screen reader tool called TalkBack that can be turned on as a system-wide setting and allows the user to get 
voice feedback from their interactions by traversing through UI elements using the d-pad on their phones (if available on the model). 
An alternative to TalkBack is a third-party open source tool called Spiel that allows per-application customization through JavaScript. 
Our system did not use either of these packages since they needed the phone's accessibility mode to be turned on, which intervened with 
custom accessibility objects that our code uses. Instead, we implemented a replacement that dispatches d-pad commands to traverse 
through visible UI elements, and simulates accessibility events to gather readable content at each step. This worked well for a uniform 
screen like the list of to-do tasks, but got in the way of user interaction in more complicated screens due to the simulated traversal. 
Therefore we only used the screen reader to read off the list of tasks.

\item Voice keyboard
Upon 

\item Natural language processing for command interpretation
\end{enumerate}

\section{Experimental Evaluation}
\subsection{Experimental Design}

\subsection{Results}

\subsection{Discussion}

\section{Conclusion and Future Work}
Potential improvements: Either infer more about UI elements (not just 
x/y coordinates, but inferring what elements the user is interacting with)
or record the context/screen the user was at during demonstration and only allow 
that command to be dispatched from the same context.


\end{document}
